#!/usr/bin/env python3
"""
PE-Based Malware Detection System

A machine learning system for detecting malware based on PE header characteristics.
"""

import os
import sys
import numpy as np
import pandas as pd
import pickle
import json
import hashlib
import logging
import argparse
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("pe_malware_detector.log"),
        logging.StreamHandler()
    ]
)

class PEMalwareDetector:
    """Main class for PE-based malware detection"""
    
    def __init__(self):
        self.models = None
        self.feature_names = None
        self.loaded = False
    
    def load_models(self, model_path="models/pe_models.pkl"):
        """Load trained malware detection models"""
        try:
            with open(model_path, "rb") as f:
                self.models = pickle.load(f)
            
            # Load model info if available
            try:
                with open(model_path.replace(".pkl", "_info.json"), "r") as f:
                    self.model_info = json.load(f)
            except:
                self.model_info = {"metrics": {"accuracy": "unknown"}}
            
            self.loaded = True
            logging.info(f"Models loaded successfully from {model_path}")
            return True
            
        except Exception as e:
            logging.error(f"Error loading models: {str(e)}")
            return False
    
    def prepare_features(self, data):
        """
        Prepare feature vector from PE header data
        
        Args:
            data: PE header data as dictionary or pandas row
            
        Returns:
            numpy array of features
        """
        # List of all PE header fields we expect
        # Reduced feature set (12 selected features)
        field_names = [
            'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData',
            'AddressOfEntryPoint', 'ImageBase', 'Subsystem', 'DllCharacteristics',
            'SizeOfImage', 'SizeOfHeaders', 'NumberOfSections', 'Characteristics',
            'MajorSubsystemVersion'
        ]

        
        # Store for reference
        if self.feature_names is None:
            self.feature_names = field_names
        
        # Create feature vector
        features = []
        for field in field_names:
            try:
                # Handle both dictionary and pandas Series inputs
                if isinstance(data, dict):
                    value = data.get(field, 0)
                else:  # pandas Series
                    value = data[field] if field in data else 0
                
                # Convert to appropriate numeric type
                try:
                    features.append(float(value))
                except:
                    features.append(0)
            except:
                features.append(0)
                
        return np.array(features)
    
    def predict(self, data):
        """
        Predict whether a file is malware based on its PE header data
        
        Args:
            data: PE header data as dictionary or pandas row
            
        Returns:
            Dictionary with prediction results
        """
        if not self.loaded:
            return {"error": "No models loaded", "prediction": None}
        
        try:
            # Prepare features
            features = self.prepare_features(data)
            
            # Get predictions from each model
            predictions = {}
            confidence_scores = {}
            
            for model_name, model in self.models.items():
                # Prediction (0 or 1)
                predictions[model_name] = int(model.predict([features])[0])

                # Confidence score
                if hasattr(model, 'predict_proba'):
                    confidence_scores[model_name] = float(model.predict_proba([features])[0][1])
                else:
                    confidence_scores[model_name] = float(predictions[model_name])
            
            # Combine predictions (simple majority voting)
            votes = list(predictions.values())
            final_prediction = 1 if sum(votes) >= len(votes)/2 else 0
            
            # Calculate overall confidence
            overall_confidence = sum(confidence_scores.values()) / len(confidence_scores)
            
            # Create result
            result = {
                "prediction": int(final_prediction),
                "is_malware": bool(final_prediction),
                "confidence": float(overall_confidence),
                "model_votes": {name: pred for name, pred in predictions.items()},
                "model_confidences": {name: conf for name, conf in confidence_scores.items()}
            }
            
            # Add feature importance analysis if available
            if hasattr(self.models.get('gb_model', None), 'feature_importances_'):
                # Get top contributing features
                importances = self.models['gb_model'].feature_importances_
                indices = np.argsort(importances)[::-1]
                top_features = []
                
                for i in range(min(10, len(indices))):
                    idx = indices[i]
                    if idx < len(self.feature_names):
                        feature_name = self.feature_names[idx]
                        importance = importances[idx]
                        feature_value = features[idx]
                        top_features.append({
                            "name": feature_name,
                            "importance": float(importance),
                            "value": float(feature_value)
                        })
                
                result["top_contributing_features"] = top_features
            
            return result
            
        except Exception as e:
            logging.error(f"Error during prediction: {str(e)}")
            return {
                "error": str(e),
                "prediction": None,
                "is_malware": None,
                "confidence": None
            }
    
    def load_and_predict_from_csv(self, csv_path):
        """
        Load PE header data from CSV and make predictions
        
        Args:
            csv_path: Path to CSV file with PE header data
            
        Returns:
            List of prediction results
        """
        try:
            df = pd.read_csv(csv_path)
            results = []
            
            logging.info(f"Processing {len(df)} samples from {csv_path}")
            
            for _, row in df.iterrows():
                result = self.predict(row)
                
                # Add SHA256 if available
                if 'SHA256' in row:
                    result['sha256'] = row['SHA256']
                
                results.append(result)
            
            # Calculate overall statistics
            malware_count = sum(1 for r in results if r.get('is_malware', False))
            
            logging.info(f"Results: {malware_count} malware, {len(results) - malware_count} benign")
            
            return results
            
        except Exception as e:
            logging.error(f"Error processing CSV: {str(e)}")
            return []

    def train_with_selected_features(csv_path, save_path="models/pe_models.pkl"):
        df = pd.read_csv(csv_path)

        selected_features = [
            'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData',
            'AddressOfEntryPoint', 'ImageBase', 'Subsystem', 'DllCharacteristics',
            'SizeOfImage', 'SizeOfHeaders', 'NumberOfSections', 'Characteristics',
            'MajorSubsystemVersion'
        ]

        # Label column
        target_col = 'label'  # Make sure this exists in your CSV

        # Filter data
        X = df[selected_features]
        y = df[target_col]

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Initialize models
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

        # Train models
        rf.fit(X_train, y_train)
        gb.fit(X_train, y_train)

        # Evaluate
        for model, name in zip([rf, gb], ["RandomForest", "GradientBoosting"]):
            y_pred = model.predict(X_test)
            print(f"== {name} ==")
            print("Accuracy:", accuracy_score(y_test, y_pred))
            print("Precision:", precision_score(y_test, y_pred))
            print("Recall:", recall_score(y_test, y_pred))
            print("F1 Score:", f1_score(y_test, y_pred))
            print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
            print("")

        # Save models
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, "wb") as f:
            pickle.dump({
                "rf_model": rf,
                "gb_model": gb
            }, f)

        print(f"Models saved to {save_path}")

    
    def train_model(self, csv_path, label_column='Type'):
        """
        Train a new model from labeled PE header data
        
        Args:
            csv_path: Path to CSV file with PE header data and labels
            label_column: Name of the column containing labels
            
        Returns:
            Dictionary with model metrics
        """
        try:
            logging.info(f"Loading training data from {csv_path}")
            df = pd.read_csv(csv_path)
            
            # Check if label column exists
            if label_column not in df.columns:
                raise ValueError(f"Label column '{label_column}' not found in the dataset")
            
            # Prepare features and labels
            X = []
            y = []
            
            logging.info("Extracting features from dataset")
            for _, row in df.iterrows():
                try:
                    # Create label - make sure "Malware" in Type column = 1, others = 0
                    label = 1 if str(row[label_column]).lower() in ["malware", "1", "true"] else 0
                    
                    feature_vector = self.prepare_features(row)
                    X.append(feature_vector)
                    y.append(label)
                except Exception as e:
                    logging.warning(f"Error processing row: {str(e)}")
                    continue
            
            X = np.array(X)
            y = np.array(y)
            
            logging.info(f"Dataset prepared: {X.shape[0]} samples with {X.shape[1]} features")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Create and train models
            self.models = {}
            
            logging.info("Training Gradient Boosting model")
            self.models['gb_model'] = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=3
            )
            self.models['gb_model'].fit(X_train, y_train)
            
            logging.info("Training Random Forest model")
            self.models['rf_model'] = RandomForestClassifier(
                n_estimators=100,
                max_depth=None,
                min_samples_split=2
            )
            self.models['rf_model'].fit(X_train, y_train)
            
            # Make predictions
            gb_preds = self.models['gb_model'].predict(X_test)
            rf_preds = self.models['rf_model'].predict(X_test)
            
            # Combine predictions (simple majority voting)
            ensemble_preds = np.zeros(len(y_test))
            for i in range(len(y_test)):
                votes = [gb_preds[i], rf_preds[i]]
                ensemble_preds[i] = 1 if sum(votes) >= 1 else 0
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, ensemble_preds)
            precision = precision_score(y_test, ensemble_preds)
            recall = recall_score(y_test, ensemble_preds)
            f1 = f1_score(y_test, ensemble_preds)
            
            # Confusion matrix
            cm = confusion_matrix(y_test, ensemble_preds)
            tn, fp, fn, tp = cm.ravel()
            
            logging.info(f"Model training completed")
            logging.info(f"Accuracy: {accuracy:.4f}")
            logging.info(f"Precision: {precision:.4f}")
            logging.info(f"Recall: {recall:.4f}")
            logging.info(f"F1 Score: {f1:.4f}")
            logging.info(f"True Negatives: {tn}, False Positives: {fp}")
            logging.info(f"False Negatives: {fn}, True Positives: {tp}")
            
            # Save feature importances
            feature_importances = self.models['gb_model'].feature_importances_
            top_features = []
            indices = np.argsort(feature_importances)[::-1]
            
            for i in range(min(20, len(indices))):
                idx = indices[i]
                feature_name = self.feature_names[idx] if idx < len(self.feature_names) else f"Feature_{idx}"
                top_features.append((feature_name, float(feature_importances[idx])))
            
            # Save models
            os.makedirs("models", exist_ok=True)
            with open("models/pe_models.pkl", "wb") as f:
                pickle.dump(self.models, f)
            
            # Save model info
            self.model_info = {
                "metrics": {
                    "accuracy": float(accuracy),
                    "precision": float(precision),
                    "recall": float(recall),
                    "f1": float(f1),
                    "confusion_matrix": {
                        "true_negatives": int(tn),
                        "false_positives": int(fp), 
                        "false_negatives": int(fn),
                        "true_positives": int(tp)
                    }
                },
                "feature_importances": {
                    name: importance for name, importance in top_features
                },
                "training_samples": len(X_train),
                "test_samples": len(X_test)
            }
            
            with open("models/pe_model_info.json", "w") as f:
                json.dump(self.model_info, f, indent=2)
            
            # Print accuracy information to console
            print("\n‚úÖ Model Training Results:")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"Precision: {precision:.4f}")
            print(f"Recall: {recall:.4f}")
            print(f"F1 Score: {f1:.4f}")
            print(f"\nConfusion Matrix:")
            print(f"True Negatives: {tn}, False Positives: {fp}")
            print(f"False Negatives: {fn}, True Positives: {tp}")
            
            print("\nTop 10 features for malware detection:")
            for name, importance in list(sorted(top_features, key=lambda x: x[1], reverse=True))[:10]:
                print(f"- {name}: {importance:.4f}")
            
            self.loaded = True
            return self.model_info
            
        except Exception as e:
            logging.error(f"Error during model training: {str(e)}")
            return {"error": str(e)}
    
    def generate_report(self, results, output_path="malware_report.json"):
        """
        Generate a detailed report from prediction results
        
        Args:
            results: List of prediction results
            output_path: Path to save the report
            
        Returns:
            Dictionary with report summary
        """
        try:
            # Count malware and benign files
            malware_count = sum(1 for r in results if r.get('is_malware', False))
            benign_count = sum(1 for r in results if r.get('is_malware') is False)
            error_count = sum(1 for r in results if 'error' in r)
            
            # Calculate confidence statistics
            confidence_values = [r.get('confidence', 0) for r in results if 'confidence' in r]
            avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0
            
            # Create summary
            summary = {
                "total_files": len(results),
                "malware_detected": malware_count,
                "benign_files": benign_count,
                "errors": error_count,
                "average_confidence": float(avg_confidence),
                "timestamp": pd.Timestamp.now().isoformat()
            }
            
            # Add model information if available
            if hasattr(self, 'model_info'):
                summary["model_metrics"] = self.model_info.get("metrics", {})
            
            # Complete report
            report = {
                "summary": summary,
                "detailed_results": results
            }
            
            # Save report
            with open(output_path, "w") as f:
                json.dump(report, f, indent=2)
            
            logging.info(f"Report saved to {output_path}")
            return summary
            
        except Exception as e:
            logging.error(f"Error generating report: {str(e)}")
            return {"error": str(e)}

def main():
    parser = argparse.ArgumentParser(description='PE-Based Malware Detection System')
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Train command
    train_parser = subparsers.add_parser('train', help='Train a new malware detection model')
    train_parser.add_argument('--dataset', required=True, help='Path to CSV file with PE header data and labels')
    train_parser.add_argument('--label-column', default='Type', help='Column name containing labels (malware vs benign)')
    
    # Predict single file
    predict_parser = subparsers.add_parser('predict', help='Analyze a single CSV file with PE header data')
    predict_parser.add_argument('--file', required=True, help='Path to CSV file to analyze')
    predict_parser.add_argument('--output', default='predictions.json', help='Path to save results')
    
    args = parser.parse_args()
    
    # Create detector
    detector = PEMalwareDetector()
    
    if args.command == 'train':
        logging.info(f"Training new model on {args.dataset}")
        metrics = detector.train_with_selected_features(args.dataset, args.label_column)
        
        if 'error' in metrics:
            print(f"\n‚ùå Error during training: {metrics['error']}")
    
    elif args.command == 'predict':
        # Load model
        if not detector.load_models():
            print("\n‚ùå Error: No trained model found. Run 'train' command first.")
            return
        
        logging.info(f"Analyzing file: {args.file}")
        results = detector.load_and_predict_from_csv(args.file)
        
        if results:
            # Generate and display report
            summary = detector.generate_report(results, args.output)
            
            print(f"\nüìä Analysis Summary:")
            print(f"Total files analyzed: {summary['total_files']}")
            print(f"Malware detected: {summary['malware_detected']}")
            print(f"Benign files: {summary['benign_files']}")
            
            if summary['malware_detected'] > 0:
                print("\n‚ö†Ô∏è  Detected Malware:")
                for idx, result in enumerate(results):
                    if result.get('is_malware', False):
                        sha = result.get('sha256', f"Sample_{idx}")
                        conf = result.get('confidence', 0) * 100
                        print(f"- {sha[:16]}... ({conf:.2f}% confidence)")
            
            print(f"\nDetailed results saved to {args.output}")
        else:
            print("\n‚ùå Error analyzing file or no samples found")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()